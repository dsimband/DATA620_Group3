{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA620: Assignment 10 - Document Classification\n",
    "\n",
    "## Homework Team 3: David Simbandumwe, Eric Lehmphul and Lidiia Tronina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to be able to classify new \"test\" documents using already classified \"training\" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  Here is one example of such data: [UCI Machine Learning Repository: Spambase Data Set](http://archive.ics.uci.edu/ml/datasets/Spambase)\n",
    "For this project, you can either use the above dataset to predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).\n",
    "For more adventurous students, you are welcome to come up a different set of documents that have already been classified , then analyze these documents to predict how new documents should be classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading inagural: Package 'inagural' not found in\n",
      "[nltk_data]     index\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lidiiatronina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import nltk\n",
    "from nltk.corpus import inaugural\n",
    "from operator import itemgetter\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('inagural')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose a corpus of interest for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presidents’ words matter. For better or worse, presidential rhetoric tells the American people who they are. That's why we decided to look at the inaugural speeches from the freely available library that can be downloaded from the NLTK package. The corpus is a collection of 55 texts, one for each presidential address.\n",
    "Let's look at available texts in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt',\n",
       " '1829-Jackson.txt',\n",
       " '1833-Jackson.txt',\n",
       " '1837-VanBuren.txt',\n",
       " '1841-Harrison.txt',\n",
       " '1845-Polk.txt',\n",
       " '1849-Taylor.txt',\n",
       " '1853-Pierce.txt',\n",
       " '1857-Buchanan.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1865-Lincoln.txt',\n",
       " '1869-Grant.txt',\n",
       " '1873-Grant.txt',\n",
       " '1877-Hayes.txt',\n",
       " '1881-Garfield.txt',\n",
       " '1885-Cleveland.txt',\n",
       " '1889-Harrison.txt',\n",
       " '1893-Cleveland.txt',\n",
       " '1897-McKinley.txt',\n",
       " '1901-McKinley.txt',\n",
       " '1905-Roosevelt.txt',\n",
       " '1909-Taft.txt',\n",
       " '1913-Wilson.txt',\n",
       " '1917-Wilson.txt',\n",
       " '1921-Harding.txt',\n",
       " '1925-Coolidge.txt',\n",
       " '1929-Hoover.txt',\n",
       " '1933-Roosevelt.txt',\n",
       " '1937-Roosevelt.txt',\n",
       " '1941-Roosevelt.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1949-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1965-Johnson.txt',\n",
       " '1969-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1977-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '2001-Bush.txt',\n",
       " '2005-Bush.txt',\n",
       " '2009-Obama.txt',\n",
       " '2013-Obama.txt',\n",
       " '2017-Trump.txt',\n",
       " '2021-Biden.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 59 Presidential inaugural speeches. We want to classify presidential speeches as Republican/Democrat. Most presidents have belonged to one of these two parties (except some who were Whigs and National Union), and we can start as early as Andrew Jackson's 1829 speech. It will be interesting to see if the two parties have different word use in their addresses and if we can identify the president's party by word choices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>president</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>Washington</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>Washington</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>Adams</td>\n",
       "      <td>Federalist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>Jefferson</td>\n",
       "      <td>Democrat-Republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>Jefferson</td>\n",
       "      <td>Democrat-Republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>Madison</td>\n",
       "      <td>Democrat-Republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>Madison</td>\n",
       "      <td>Democrat-Republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>Monroe</td>\n",
       "      <td>Democrat-Republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>Monroe</td>\n",
       "      <td>Democrat-Republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>Adams</td>\n",
       "      <td>Democrat-Republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>Jackson</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>Jackson</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>VanBuren</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>Harrison</td>\n",
       "      <td>Whig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845</th>\n",
       "      <td>Polk</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>Taylor</td>\n",
       "      <td>Whig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>Pierce</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1857</th>\n",
       "      <td>Buchanan</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>Republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>Republican</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       president                party\n",
       "year                                 \n",
       "1789  Washington                 None\n",
       "1793  Washington                 None\n",
       "1797       Adams           Federalist\n",
       "1801   Jefferson  Democrat-Republican\n",
       "1805   Jefferson  Democrat-Republican\n",
       "1809     Madison  Democrat-Republican\n",
       "1813     Madison  Democrat-Republican\n",
       "1817      Monroe  Democrat-Republican\n",
       "1821      Monroe  Democrat-Republican\n",
       "1825       Adams  Democrat-Republican\n",
       "1829     Jackson             Democrat\n",
       "1833     Jackson             Democrat\n",
       "1837    VanBuren             Democrat\n",
       "1841    Harrison                 Whig\n",
       "1845        Polk             Democrat\n",
       "1849      Taylor                 Whig\n",
       "1853      Pierce             Democrat\n",
       "1857    Buchanan             Democrat\n",
       "1861     Lincoln           Republican\n",
       "1865     Lincoln           Republican"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "party = pd.read_csv('https://raw.githubusercontent.com/dsimband/DATA620_Group3/main/Week10_Assignment/presidents.csv',index_col=0)\n",
    "\n",
    "party.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 152901 words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152901"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count ALL words\n",
    "all_words = inaugural.words()\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Washington's speech\n",
    "inaugural.words('1789-Washington.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 9555),\n",
       " (',', 7275),\n",
       " ('of', 7169),\n",
       " ('and', 5226),\n",
       " ('.', 5011),\n",
       " ('to', 4477),\n",
       " ('in', 2604),\n",
       " ('a', 2229),\n",
       " ('our', 2062),\n",
       " ('that', 1769)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(all_words).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10025 unique words in this data. Looking at the sample of our words above, we can see that it includes punctuation as well as stopwords, such as 'the' and 'of'. These words are meaningless for research. We also know that python will see capital letters as distinct from lowercase letters, so we need to convert all words to lowercase and remove punctuation, some common words, and numbers to get only the unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10025"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords can be done by calling the stopwords corpus in NLTK, which contain the common\n",
    "high-frequency words with no practical meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#remove stopwords \n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = [word.lower() for word in all_words if word.lower() not in stop_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80124"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9173"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(filtered_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "custom_stopwords = set((',',' ','.', ';', '?', '-', '!', '(', ')','--','\"',\"'\", ':', '¡¦', '¡','', '9', '/', '11','ii', '400','1863','','us',',\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words2 = [ word.lower() for word in filtered_words if word.lower() not in custom_stopwords ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65255"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9150"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(filtered_words2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we filtered out all unnecessary words, we got 9150 unique words in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 200 highest frequency words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('government', 600),\n",
       " ('people', 594),\n",
       " ('must', 374),\n",
       " ('upon', 371),\n",
       " ('great', 346),\n",
       " ('world', 346),\n",
       " ('may', 343),\n",
       " ('states', 335),\n",
       " ('nation', 330),\n",
       " ('country', 322),\n",
       " ('shall', 316),\n",
       " ('every', 301),\n",
       " ('one', 272),\n",
       " ('peace', 259),\n",
       " ('new', 255),\n",
       " ('citizens', 248),\n",
       " ('power', 241),\n",
       " ('america', 240),\n",
       " ('public', 227),\n",
       " ('time', 223),\n",
       " ('would', 213),\n",
       " ('constitution', 209),\n",
       " ('united', 204),\n",
       " ('nations', 199),\n",
       " ('union', 191),\n",
       " ('freedom', 189),\n",
       " ('war', 185),\n",
       " ('free', 184),\n",
       " ('american', 171),\n",
       " ('let', 160),\n",
       " ('fellow', 158),\n",
       " ('national', 158),\n",
       " ('made', 156),\n",
       " ('good', 150),\n",
       " ('men', 149),\n",
       " ('make', 147),\n",
       " ('years', 143),\n",
       " ('well', 142),\n",
       " ('justice', 142),\n",
       " ('life', 140),\n",
       " ('without', 140),\n",
       " ('spirit', 140),\n",
       " ('rights', 138),\n",
       " ('never', 137),\n",
       " ('law', 136),\n",
       " ('congress', 130),\n",
       " ('laws', 130),\n",
       " ('work', 124),\n",
       " ('liberty', 123),\n",
       " ('right', 122),\n",
       " ('best', 122),\n",
       " ('duty', 120),\n",
       " ('hope', 120),\n",
       " ('interests', 115),\n",
       " ('know', 112),\n",
       " ('god', 112),\n",
       " ('today', 112),\n",
       " ('much', 111),\n",
       " ('many', 110),\n",
       " ('state', 110),\n",
       " ('among', 108),\n",
       " ('political', 107),\n",
       " ('history', 106),\n",
       " ('foreign', 104),\n",
       " ('long', 103),\n",
       " ('first', 102),\n",
       " ('man', 102),\n",
       " ('powers', 101),\n",
       " ('day', 98),\n",
       " ('future', 97),\n",
       " ('executive', 97),\n",
       " ('policy', 97),\n",
       " ('president', 96),\n",
       " ('principles', 96),\n",
       " ('progress', 96),\n",
       " ('part', 96),\n",
       " ('human', 94),\n",
       " ('within', 94),\n",
       " ('whole', 93),\n",
       " ('ever', 93),\n",
       " ('common', 93),\n",
       " ('duties', 92),\n",
       " ('support', 92),\n",
       " ('administration', 91),\n",
       " ('faith', 91),\n",
       " ('system', 90),\n",
       " ('far', 89),\n",
       " ('together', 89),\n",
       " ('confidence', 88),\n",
       " ('service', 87),\n",
       " ('purpose', 87),\n",
       " ('present', 86),\n",
       " ('way', 86),\n",
       " ('less', 85),\n",
       " ('still', 84),\n",
       " ('yet', 84),\n",
       " ('americans', 84),\n",
       " ('necessary', 82),\n",
       " ('party', 82),\n",
       " ('always', 81),\n",
       " ('come', 81),\n",
       " ('better', 80),\n",
       " ('force', 80),\n",
       " ('strength', 80),\n",
       " ('high', 78),\n",
       " ('believe', 78),\n",
       " ('need', 78),\n",
       " ('old', 78),\n",
       " ('equal', 77),\n",
       " ('institutions', 77),\n",
       " ('interest', 77),\n",
       " ('others', 76),\n",
       " ('office', 75),\n",
       " ('general', 75),\n",
       " ('.\"', 74),\n",
       " ('things', 74),\n",
       " ('land', 73),\n",
       " ('could', 72),\n",
       " ('means', 72),\n",
       " ('even', 72),\n",
       " ('home', 72),\n",
       " ('place', 71),\n",
       " ('federal', 71),\n",
       " ('democracy', 71),\n",
       " ('prosperity', 70),\n",
       " ('give', 70),\n",
       " ('change', 70),\n",
       " ('might', 69),\n",
       " ('secure', 69),\n",
       " ('action', 69),\n",
       " ('another', 68),\n",
       " ('like', 68),\n",
       " ('find', 68),\n",
       " ('republic', 68),\n",
       " ('security', 67),\n",
       " ('order', 66),\n",
       " ('whose', 66),\n",
       " ('given', 66),\n",
       " ('take', 66),\n",
       " ('full', 66),\n",
       " ('stand', 65),\n",
       " ('responsibility', 65),\n",
       " ('self', 65),\n",
       " ('called', 64),\n",
       " ('important', 64),\n",
       " ('proper', 64),\n",
       " ('found', 64),\n",
       " ('commerce', 64),\n",
       " ('see', 63),\n",
       " ('true', 63),\n",
       " ('business', 63),\n",
       " ('subject', 62),\n",
       " ('respect', 62),\n",
       " ('honor', 62),\n",
       " ('cause', 62),\n",
       " ('earth', 62),\n",
       " ('seek', 62),\n",
       " ('revenue', 61),\n",
       " ('century', 61),\n",
       " ('trust', 60),\n",
       " ('ought', 60),\n",
       " ('character', 60),\n",
       " ('since', 60),\n",
       " ('question', 60),\n",
       " ('children', 60),\n",
       " ('principle', 59),\n",
       " ('toward', 59),\n",
       " ('civil', 58),\n",
       " ('oath', 58),\n",
       " ('beyond', 58),\n",
       " ('done', 58),\n",
       " ('act', 57),\n",
       " ('influence', 57),\n",
       " ('course', 57),\n",
       " ('become', 57),\n",
       " ('protection', 57),\n",
       " ('cannot', 57),\n",
       " ('love', 56),\n",
       " ('meet', 56),\n",
       " ('citizen', 56),\n",
       " ('strong', 56),\n",
       " ('also', 56),\n",
       " ('help', 56),\n",
       " ('past', 55),\n",
       " ('however', 55),\n",
       " ('authority', 55),\n",
       " ('live', 55),\n",
       " ('countrymen', 55),\n",
       " ('greater', 54),\n",
       " ('use', 54),\n",
       " ('constitutional', 53),\n",
       " ('economy', 53),\n",
       " ('happiness', 52),\n",
       " ('experience', 52),\n",
       " ('sense', 52),\n",
       " ('individual', 52),\n",
       " ('promise', 52),\n",
       " ('hand', 51),\n",
       " ('governments', 51),\n",
       " ('times', 51)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(filtered_words2)\n",
    "fdist.most_common(200) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
